---
title: "EHR Vignette for Structured Data"
date: '`r Sys.Date()`'
output: pdf_document
---

<!--
%\VignetteEngine{knitr::rmarkdown}
%\VignetteIndexEntry{2. EHR Vignette for Structured Data}
%\VignetteEncoding{UTF-8}
-->

```{r setup, include = FALSE}
knitr::opts_chunk$set(echo = TRUE)

library(EHR)
library(lubridate)
library(pkdata)
options(stringsAsFactors = FALSE)
```

```{r niceroutput, echo = FALSE}
findbreaks <- function(x, char = '[ /\\]', charlen = 75) {
  if(length(x) > 1) {
    out <- vapply(x, findbreaks, character(1), char, charlen, USE.NAMES = FALSE)
    ix <- !grepl('\n[[:space:]]*$', out)
    ix[length(ix)] <- FALSE
    out[ix] <- paste0(out[ix], '\n')
    return(paste(out, collapse = ''))
  }
  cur <- x
  nbuf <- ceiling(nchar(x) / charlen)
  if(nbuf == 1) {
    return(cur)
  }
  strings <- character(nbuf)
  i <- 1
  while(nchar(cur) > charlen) {
    loc <- c(gregexpr(char, cur)[[1]])
    b <- loc[max(which(loc < charlen))]
    strings[i] <- substr(cur, 1, b)
    cur <- substring(cur, b + 1)
    i <- i + 1
  }
  strings[i] <- cur
  paste(c(strings[1], paste0('     ', strings[-1])), collapse = '\n')
}

co <- function(expr) {
  txt <- capture.output(expr)
  cat(findbreaks(txt))
  cat("\n")
}
```

# Introduction

The `EHR` package provides several modules to perform diverse medication-related studies using data from electronic health record (EHR) databases. Especially, the package includes modules to perform pharmacokinetic/pharmacodynamic (PK/PD) analyses using EHRs, as outlined in Choi *et al.*$^{1}$, and additional modules will be added in the future. This vignette describes four modules for processing data (*Pro-Demographic*, *Pro-Med-Str*, *Pro-Drug Level*, *Pro-Laboratory*) and one module for PK data building (*Build-PK-IV*) in the system, when data are typically obtained from a structured database.

The process starts with structured data extracted by Structured Query Language (SQL) from EHRs or provided by a user, then moves through two phases: data processing which standardizes and combines the input data (*Pro-Med-Str*, *Pro-Drug Level*, etc.) and data building which creates the final PK data (*Build-PK-IV*).

The vignette has two examples. The first example demonstrates how to build PK data quickly without using the data processing modules when cleaned data for concentration, drug dose, demographic and laboratory datasets are already available in an appropriate data form. The second example shows how to utilize several data processing modules to standardize and combine more complex datasets, and then finally build PK data using *Build-PK-IV*.

To begin we load the `EHR` package, the `pkdata` package, and the `lubridate` package. 

```{r load-lib-dir, eval=FALSE}
# load EHR package and dependencies
library(EHR)
library(pkdata)
library(lubridate)
```

# Example 1: Quick Data Building with Processed Datasets

The data for example 1 includes a demographic file, a concentration file, an intravenous (IV) dosing file, and a laboratory file, which are all cleaned and formatted appropriately. We also define a directory for the raw data and a directory for interactive checking output files.

```{r simp-in, cache=TRUE}
# define directories
rawDataDir <- system.file("examples", "str_ex1", package="EHR")
checkDir <- system.file("examples", "str_ex1", "checks", package="EHR")

demo <- read.csv(file.path(rawDataDir,"Demographics_DATA_simple.csv"))
head(demo)

conc.data <- read.csv(file.path(rawDataDir,"Concentration_DATA_simple.csv"))
head(conc.data)

ivdose.data <- read.csv(file.path(rawDataDir,"IVDose_DATA_simple.csv"))
head(ivdose.data)

creat.data <- read.csv(file.path(rawDataDir,"Creatinine_DATA_simple.csv"))
head(creat.data)
```

The `EHR` package modules use a standardized naming convention for patient identification (ID) variables. We rename the unique patient-level ID from `patient_id` to `mod_id` and the visit-level ID from `patient_visit_id` to `mod_id_visit`. If there is only a single visit/course per subject the unique patient-level ID and visit-level ID can be the same, however both `mod_id` and `mod_id_visit` should be defined.

```{r simp-rename, cache=TRUE}
names(conc.data)[1:2] <- names(demo)[1:2] <- c("mod_id", "mod_id_visit")
names(creat.data)[1] <- names(ivdose.data)[1] <- "mod_id"
```

Using the four datasets, we can build a final PK dataset with the function `run_Build_PK_IV()`. Additional details for this function are provided below in the *Build-PK-IV* subsection of Example 2: Complete Data Processing and Building from Raw Extracted Data to PK Data.

```{r sim-build-pk-iv, eval = FALSE}
simple_pk_dat <- run_Build_PK_IV(
    conc=conc.data,
    dose=ivdose.data,
    lab.dat = list(creat.data),
    lab.vars = c('creat'),
    demo.list = demo,
    demo.vars=c('weight', 'weight_demo', 'height', 'gender', 'ageatsurgery',
                'stat_sts', 'cpb_sts', 'length_of_icu_stay'),
    demo.abbr=c('wgt', 'wgt_demo', 'height', 'gender',
                'age', 'stat', 'cpb', 'loi'),
    pk.vars=c('mod_id_visit', 'time', 'conc', 'dose', 'rate', 'event',
              'other', 'multiple.record', 'date', 'mod_id'),
    drugname='fent',
    check.path=checkDir)
```

```{r sim-build-pk-iv-fake, cache=TRUE, echo = FALSE}
co({
simple_pk_dat <- run_Build_PK_IV(
    conc=conc.data,
    dose=ivdose.data,
    lab.dat = list(creat.data),
    lab.vars = c('creat'),
    demo.list = demo,
    demo.vars=c('weight', 'weight_demo', 'height', 'gender', 'ageatsurgery',
                'stat_sts', 'cpb_sts', 'length_of_icu_stay'),
    demo.abbr=c('wgt', 'wgt_demo', 'height', 'gender',
                'age', 'stat', 'cpb', 'loi'),
    pk.vars=c('mod_id_visit', 'time', 'conc', 'dose', 'rate', 'event',
              'other', 'multiple.record', 'date', 'mod_id'),
    drugname='fent',
    check.path=checkDir)
})
```

```{r sim-build-pk-iv-out}
head(simple_pk_dat,15)
```

# Example 2: Complete Data Processing and Building from Raw Extracted Data to PK Data 

To begin example 2 we define three directories for the raw data, the processed data, and files used for interactive checking.

```{r ex2-dirs}
rawDataDir <- system.file("examples", "str_ex2", package="EHR")
dataDir <- system.file("examples", "str_ex2", package="EHR")
checkDir <- system.file("examples", "str_ex2", "checks", package="EHR")
```

## Pre-Processing for Raw Extracted Data

The raw data for example 2 includes a demographic file for use with the *Pro-Demographic* module; two files for the *Pro-Drug Level* module; two dosing files for the *Pro-Med-Str* module; and two lab files for use with the *Pro-Laboratory* module.

The structured datasets extracted by SQL must go through a pre-processing stage which creates new ID variables and datasets that can be used by the data processing modules. The following annotated example demonstrates the three main steps of pre-processing: (1) read and clean raw data; (2) merge raw data to create new ID variables; (3) make new data for use with modules.

Each raw dataset should contain a subject unique ID, a subject visit ID, or both ids. In this example the subject unique ID is called `subject_uid` and the subject visit ID is called `subject_id`. The subject visit ID is a combination of subject and visit/course -- e.g., `subject_id` 14.0 is the first course for subject 14, `subject_id` 14.1 is the second course for subject 14, and so on. `subject_uid` is a unique ID that is the same for all subject records. The integer part of `subject_id` has a 1-to-1 correspondence with `subject_uid` -- for this example, `subject_uid` 62734832 is associated with both `subject_id` 14.0 and `subject_id` 14.1. If there is only a single visit/course per subject only the subject unique ID is needed.

### (1) Read and clean raw data

The demographics data file contains ID variables `subject_id` and `subject_uid`, in addition to demographic variables such as gender, date of birth, height, weight, etc. The Demographics_DATA.csv file is read in using the `readTransform()` function. 

```{r demo-in, cache=TRUE}
# demographics data
demo.in <- readTransform(file.path(rawDataDir, "Demographics_DATA.csv"))
head(demo.in)
```

The example concentration data consists of two files, SampleTimes_DATA.csv and SampleConcentration_DATA.csv containing the concentration sampling times and values, respectively.

The sampling times data csv file is read in with `read.csv()`. Then the function `dataTransformation()` is used to rename the variable `Study.ID` to `subject_id` and to create a new variable called `samp`, which indexes the sample number, using the `modify=` argument. 

```{r samp-in1, cache=TRUE}
# concentration sampling times data
# read in raw data
samp.raw <- read.csv(file.path(rawDataDir, "SampleTimes_DATA.csv"))
head(samp.raw)

# transform data
samp.in0 <- dataTransformation(samp.raw,
    rename = c('Study.ID' = 'subject_id'),
    modify = list(samp = expression(as.numeric(sub('Sample ', '', Event.Name)))))
head(samp.in0)
```

Equivalently, the function `readTransform()` can be used to read in and transform the data with a single function call.

```{r samp-in2, cache=TRUE}
# read in and transform data
samp.in <- readTransform(file.path(rawDataDir, "SampleTimes_DATA.csv"),
    rename = c('Study.ID' = 'subject_id'),
    modify = list(samp = expression(as.numeric(sub('Sample ', '', Event.Name)))))
head(samp.in)
```

The same steps can be used for the sample values data csv file. It is read in using `read.csv()`. Then using `dataTransformation()` the `subject_id` variable is created from the `name` variable using a call to the helper function `sampId()` in the `modify=` argument.

```{r conc-in1, cache=TRUE}
# concentration sample values data
# read in raw data
conc.raw<-read.csv(file.path(rawDataDir, "SampleConcentration_DATA.csv"))
head(conc.raw)

# helper function used to make subject_id
sampId <- function(x) {
  # remove leading zeroes or trailing periods
  subid <- gsub('(^0*|\\.$)', '', x)
  # change _ to .
  gsub('_([0-9]+[_].*)$', '.\\1', subid)
}

# transform data
conc.in0 <- dataTransformation(conc.raw,
                    modify = list(
                    subid = expression(sampId(name)),
                    subject_id = expression(as.numeric(sub('[_].*', '', subid))),
                    samp = expression(sub('[^_]*[_]', '', subid)),
                    name = NULL,
                    data_file = NULL,
                    subid = NULL
                    )
                  )
head(conc.in0)
```

Again, we can perform the same two steps with a single call to `readTransform()`.

```{r conc-in2, cache=TRUE}
# equivalent using readTransform()
conc.in <- readTransform(file.path(rawDataDir, "SampleConcentration_DATA.csv"),
  modify = list(
    subid = expression(sampId(name)),
    subject_id = expression(as.numeric(sub('[_].*', '', subid))),
    samp = expression(sub('[^_]*[_]', '', subid)),
    name = NULL,
    data_file = NULL,
    subid = NULL
    )
  )
head(conc.in)
```

The example drug dosing data consists of files FLOW_DATA.csv and MAR_DATA.csv containing two sources of IV dose information. The FLOW data csv file contains aliases for both ID variables; it is read in with the `readTransform()` function which renames the variables `Subject.Id` to `subject_id` and `Subject.Uniq.Id` to `subject_uid`. 

```{r flow-in, cache=TRUE}
# FLOW dosing data
flow.in <- readTransform(file.path(rawDataDir, "FLOW_DATA.csv"),
                         rename = c('Subject.Id' = 'subject_id',
                                    'Subject.Uniq.Id' = 'subject_uid')) 
head(flow.in)
```

The MAR data csv file contains several variables with a colon (:) character. To preserve the colon in these variable names, the data can be read in without checking for syntactically valid `R` variable names. The data is read in using `read.csv()` with the argument `check.names = FALSE` and then passed to the `dataTransformation()` function which renames `Uniq.Id` to `subject_uid`.

```{r mar-in, cache=TRUE}
# MAR dosing data
mar.in0 <- read.csv(file.path(rawDataDir, "MAR_DATA.csv"), check.names = FALSE)
mar.in <- dataTransformation(mar.in0, rename = c('Uniq.Id' = 'subject_uid'))
head(mar.in)
```

The example laboratory data consists of files Creatinine_DATA.csv and Albumin_DATA.csv. Both files are read in using the `readTransform()` function and `Subject.uniq` is renamed to `subject_uid`.

```{r labs-in, cache=TRUE}
# Serum creatinine lab data
creat.in <- readTransform(file.path(rawDataDir, "Creatinine_DATA.csv"),
    rename = c('Subject.uniq' = 'subject_uid'))
head(creat.in)

# Albumin lab data
alb.in <- readTransform(file.path(rawDataDir, "Albumin_DATA.csv"),
    rename = c('Subject.uniq' = 'subject_uid'))
head(creat.in)
```

### (2) Merge data to create new ID variables

The function `idCrosswalk()` merges all of the cleaned input datasets and creates new IDs. The `data=` argument of this function accepts a list of input datasets and the `idcols=` argument accepts a list of vectors or character strings that identify the ID variables in the corresponding input dataset. 

The output of `idCrosswalk()` is a crosswalk dataset between the original ID variables (`subject_id`, `subject_uid`) and the new ID variables (`mod_id`, `mod_visit`, and `mod_id_visit`). The new variable `mod_id_visit` has a 1-to-1 correspondence to variable `subject_id` and uniquely identifies each subjects' visit/course; the new variable `mod_id` has a 1-to-1 correspondence to variable `subject_uid` and uniquely identifies each subject.

```{r merge-ids, cache=TRUE}
# merge all ID datasets
data <-  list(demo.in,
              samp.in,
              conc.in,
              flow.in,
              mar.in,
              creat.in,
              alb.in)

idcols <-  list(c('subject_id', 'subject_uid'), # id vars in demo.in
                'subject_id', # id var in samp.in
                'subject_id', # id var in conc.in
                c('subject_id', 'subject_uid'), # id vars in flow.in
                'subject_uid', # id var in mar.in
                'subject_uid', # id var in creat.in
                'subject_uid') # id var in creat.in

mod.id <- idCrosswalk(data, idcols, visit.id="subject_id", uniq.id="subject_uid")
saveRDS(mod.id, file=file.path(dataDir,"Fentanyl_module_id.rds"))

mod.id
```

### (3) Make new data for use with modules

The function `pullFakeId()` replaces the original IDs -- `subject_id` and `subject_uid` -- with new IDs -- `mod_id`, `mod_visit`, and `mod_id_visit` -- to create datasets which can be used by the data processing modules. Generally, the function call to `pullFakeId()` is

```{r, eval = FALSE}
pullFakeId(dat, xwalk, firstCols = NULL, orderBy = NULL)
```

The `dat=` argument should contain the cleaned input data.frame from pre-processing step (1) and the `xwalk=` argument should contain the crosswalk data.frame produced in step (2). Additional arguments `firstCols=` and `orderBy=` control which variables are in the first columns of the output and the sort order, respectively. The cleaned structured data are saved as `R` objects for use with the modules.

```{r mod-id-data, cache=TRUE}
## demographics data
demo.cln <- pullFakeId(demo.in, mod.id,
    firstCols = c('mod_id', 'mod_visit', 'mod_id_visit'),
    uniq.id = 'subject_uid')
head(demo.cln)
saveRDS(demo.cln, file=file.path(dataDir,"Fentanyl_demo_mod_id.rds"))

## drug level data
# sampling times
samp.cln <- pullFakeId(samp.in, mod.id,
    firstCols = c('mod_id', 'mod_visit', 'mod_id_visit', 'samp'), 
    orderBy = c('mod_id_visit','samp'),
    uniq.id = 'subject_uid')
head(samp.cln)
saveRDS(samp.cln, file=file.path(dataDir,"Fentanyl_samp_mod_id.rds"))

# sampling concentrations
conc.cln <- pullFakeId(conc.in, mod.id,
    firstCols = c('record_id', 'mod_id', 'mod_visit', 'mod_id_visit', 'samp'),
    orderBy = 'record_id',
    uniq.id = 'subject_uid')
head(conc.cln)
saveRDS(conc.cln, file=file.path(dataDir,"Fentanyl_conc_mod_id.rds"))

## dosing data
# flow
flow.cln <- pullFakeId(flow.in, mod.id,
    firstCols = c('mod_id', 'mod_visit', 'mod_id_visit'),
    uniq.id = 'subject_uid')
head(flow.cln)
saveRDS(flow.cln, file=file.path(dataDir,"Fentanyl_flow_mod_id.rds"))

# mar
mar.cln <- pullFakeId(mar.in, mod.id, firstCols = 'mod_id', uniq.id = 'subject_uid')
head(mar.cln)
saveRDS(mar.cln, file=file.path(dataDir,"Fentanyl_mar_mod_id.rds"))

## laboratory data
creat.cln <- pullFakeId(creat.in, mod.id, 'mod_id',uniq.id = 'subject_uid')
head(creat.cln)

alb.cln <- pullFakeId(alb.in, mod.id, 'mod_id', uniq.id = 'subject_uid')
head(alb.cln)

saveRDS(creat.cln, file=file.path(dataDir,"Fentanyl_creat_mod_id.rds"))
saveRDS(alb.cln, file=file.path(dataDir,"Fentanyl_alb_mod_id.rds"))
```

Before running the processing modules, it is necessary to define several options and parameters. Using `options(pkxwalk =)` allows the modules to access the crosswalk file. We also create a `drugname` stub and define the lower limit of quantification (LLOQ) for the drug of interest.

```{r mod-setup}
# set crosswalk option 
xwalk <- readRDS(file.path(dataDir, "Fentanyl_module_id.rds"))
options(pkxwalk = 'xwalk')

# define parameters
drugname <- 'fent'
LLOQ <- 0.05
```


## Pro-Demographic

The *Pro-Demographic* module accepts the cleaned structured demographic dataset and a user-defined set of exclusion criteria and returns a formatted list with the demographic data and records meeting the exclusion criteria suitable for integration with the other modules. For this example, we exclude subjects with a value of 1 for `in_hospital_mortality` or `add_ecmo` and create a new variable called `length_of_icu_stay`.

The demographic data can be processed by the `run_Demo()` function using:

```{r Pro-Demographic, cache=TRUE, eval=TRUE}
# helper function
exclude_val <- function(x, val=1) { !is.na(x) & x == val }

demo.out <- run_Demo(demo.path = file.path(dataDir, "Fentanyl_demo_mod_id.rds"),
    toexclude = expression(exclude_val(in_hospital_mortality) | exclude_val(add_ecmo)),
    demo.mod.list = list(length_of_icu_stay = 
                        expression(daysDiff(surgery_date, date_icu_dc))))

head(demo.out$demo)
demo.out$exclude
```

* The `run_Demo` function arguments are as follows:
    + `demo.path`: file path where cleaned demographic data exist
    + `toexclude`: (optional) a set of user-defined expressions to set exclusion rules to be applied to the demographic dataset within the module
    + `demo.mod.list`: (optional) a list of user defined expressions to set modification rules within the module

See the `run_Demo()` function documentation for more details on the optional arguments.

* The output of `run_Demo()` is a list with two components: `demo`, a dataframe containing a demogrpahic information and `exclude`, vector of excluded visit IDs.

## Pro-Med-Str

The *Pro-Med-Str* module processes structured medication data. Part I handles IV dose data and Part II handles e-prescription data.

### Part I: IV dose data

Part I handles IV dose data from two sources, Flow data and Medication Administration Records (MAR) data. The Flow data are patient flow sheets which at this institution record infusion rates and changes to all infusions for all inpatients outside of the operating room. The MAR data  record all bolus doses of medications and infusions administered in the operating room. The module is semi-interactive; it generates several files to check potential data errors and get feedback from an investigator. If corrected information ('fix' files) are provided, the module should be re-run to incorporate the corrections. The major functions of this module are:

* Process and clean Flow data by selecting, renaming, and modifying variables with the `flow.select`, `flow.rename`, and `flow.mod.list` parameters, and remove duplicate records, including invalid duplicate rows with `final.units` of 0, rate missing, one rate or unit missing, one unit of 0, or additional discrepancy.

* Process MAR data using the given medications in the `medchk.path` file, infusion units, bolus units, and bolus rate threshold. If there are data rows containing units other than those specified, a file listing these records will be produced using the `failunit_fn` stub appended with drugname.
     + e.g., if failunit_fn is 'Unit' the file 'failUnit-fent.csv' will be created in the `check.path` directory. The corrected file should replace 'fail' with 'fix' (e.g., 'fixUnit-fent.csv'), and include an additional variable called 'flag' with the value 'keep' for the records to be retained.
  
* Check for records with a unit containing 'kg' (e.g., 'mcg/kg/hr') but missing weight in flow data. These records will result in a calculated rate of NA when producing standardized rates (see step below). If there are records with missing weight, a file listing these records will be produced using the `failnowgt_fn` stub appended with drugname. A 'fix' file can be added to correct these records.

* Combine Flow and MAR infusion data and produce standardized rate (dose per unit time). Infusion rates with dose per weight per unit time (e.g., 'mcg/kg/hr') and dose per unit time ('mcg/hr') can both be handled. For infusion rates of dose per weight per unit time, weight is imputed using the closest available Flow weight (if possible) and the rate is multiplied by weight to get dose per unit time. Infusion rates of dose per unit time are assumed to be formatted correctly.

* Merge infusion dose data and bolus dose data and conform doses. See the `pkdata::conformDoses()` documentation for additional details. 

The IV dose data can be processed by the `run_MedStrI()` function using: 

```{r Pro-Med-Str1, eval=FALSE}
ivdose.out <- run_MedStrI(flow.path=file.path(dataDir,"Fentanyl_flow_mod_id.rds"), 
    flow.select = c('mod_id','mod_id_visit','Perform.Date','Final.Wt..kg.',
                    'Final.Rate..NFR.units.','Final.Units'),
    flow.rename = c('mod_id','mod_id_visit', 'Perform.Date', 'weight',
                    'rate', 'final.units'),
    flow.mod.list = list(
      date.time = expression(parse_dates(fixDates(Perform.Date))),
      unit = expression(sub('.*[ ]', '', rate)),
      rate = expression(as.numeric(sub('([0-9.]+).*', '\\1', rate)))),
    medchk.path=file.path(rawDataDir, sprintf('medChecked-%s.csv', drugname)), 
    mar.path=file.path(dataDir,"Fentanyl_mar_mod_id.rds"),
    demo.list=demo.out,
    check.path=checkDir, 
    failflow_fn = 'FailFlow',
    failunit_fn = 'Unit',
    failnowgt_fn = 'NoWgt',
    infusion.unit = 'mcg/kg/hr',
    bolus.unit = 'mcg',
    bol.rate.thresh = Inf,
    drugname = drugname)
```

```{r Pro-Med-Str1-fake, cache=TRUE, echo=FALSE}
co({
ivdose.out <- run_MedStrI(flow.path=file.path(dataDir,"Fentanyl_flow_mod_id.rds"), 
    flow.select = c('mod_id','mod_id_visit','Perform.Date','Final.Wt..kg.',
                    'Final.Rate..NFR.units.','Final.Units'),
    flow.rename = c('mod_id','mod_id_visit', 'Perform.Date', 'weight',
                    'rate', 'final.units'),
    flow.mod.list = list(
      date.time = expression(parse_dates(fixDates(Perform.Date))),
      unit = expression(sub('.*[ ]', '', rate)),
      rate = expression(as.numeric(sub('([0-9.]+).*', '\\1', rate)))),
    medchk.path=file.path(rawDataDir, sprintf('medChecked-%s.csv', drugname)), 
    mar.path=file.path(dataDir,"Fentanyl_mar_mod_id.rds"),
    demo.list=demo.out,
    check.path=checkDir, 
    failflow_fn = 'FailFlow',
    failunit_fn = 'Unit',
    failnowgt_fn = 'NoWgt',
    infusion.unit = 'mcg/kg/hr',
    bolus.unit = 'mcg',
    bol.rate.thresh = Inf,
    drugname = drugname)
    })
```

```{r Pro-Med-Str1-out}
head(ivdose.out)
```

* The `run_MedStrI` function arguments are as follows:
    + `flow.path`: file path where Flow data exist
    + `flow.select`: list of variables in the Flow data, which are used for processing
    + `flow.rename`: rename variables for the variables in flow.select
    + `flow.mod.list`: list containing modifications to variables in the Flow data
    + `medchk.path`: file path with list of medication names and variants in MAR data to be used 
    + `mar.path`: file path where the MAR data exist
    + `demo.list`: (optional) file name for processed demographic file that is used to exclude subjects based on exclusion criteria 
    + `check.path`: file path where the generated files for data checking are stored, and the corresponding data files with fixed data exist
    + `failflow_fn`: filename stub for invalid duplicate rows in the Flow data with rate of 0 check file. The stub will be prepended with the string 'fail' to create a .csv with the invalida data. The corrected data with failures replaced should use the same filename stub prepended with the string 'fix'.
         - e.g., if `failflow_fn` is 'FailFlow', the file 'failFailFlow.csv' with invalid duplicate rows will be created in the directory specified by `check.path`. The corrected version named 'fixFailFlow.csv' should be placed in the same directory.
    + `failunit_fn`: filename stub for records with units other than those specified with `infusion.unit` and `bolus.unit`
    + `failnowgt_fn`: filename stub for records with missing weight in the Flow data and unit involving 'kg'
    + `infusion.unit`: string specifying units for infusion doses (default: 'mcg/kg/hr')
    + `bolus.unit`: string specifying units for bolus doses (default: 'mcg')
    + `bol.rate.thresh`: upper bound for retaining bolus doses. Bolus units with a rate above the threshold are dropped (default: Inf; i.e., keep all bolus doses)
    + `drugname`: drug name of interest (e.g., dex, fent)
* The output of the `run_MedStrI` function is a dataset with processed IV dosing data including date (date.dose), infusion dose time (infuse.time.real and infuse.time) and rate (infuse.dose), bolus dose time (bolus.time) and dose level (bolus.dose), weight used in dose calculation and identification numbers for further merging with the output from other modules.

### Part II: e-prescription data

Part II handles e-prescription data. To use this module, all prescriptions must be for only one drug. Different names, such as brand names and generic names, for the same drug are allowed (e.g., Lamictal and lamotrigine). The data used in this module must include columns for ID, date, strength, dose amount, and frequency. The major tasks the module performs are as follows:

* Creating numeric variables for strength, dose, and frequency
* Calculating daily dose
* Removing duplicate daily doses for a patient

There are two underlying functions used in this module. `processErx` performs the basic cleaning described above. `processErxAddl` performs some additional processing for more complicated dose expressions.

Below is example e-prescription data including columns for ID, drug name, dose, frequency, date, strength, and description.

```{r eRX-dat, cache=TRUE}
(eRX <- read.csv(file.path(rawDataDir,"e-rx_DATA.csv"),stringsAsFactors = FALSE))
```

The e-prescription data can be processed by the `run_MedStrII` function using: 

```{r Pro-Med-Str2, cache=TRUE}
eRX.out <- run_MedStrII(file.path(rawDataDir,"e-rx_DATA.csv"),
    select = c('GRID','MED_NAME','RX_DOSE','FREQUENCY','ENTRY_DATE','STRENGTH_AMOUNT','DESCRIPTION'),
    rename = c('ID','MED_NAME','RX_DOSE','FREQUENCY','ENTRY_DATE','STRENGTH_AMOUNT','DESCRIPTION'))

eRX.out
```

The following arguments are used in the `run_MedStrII` function:

* `file`: file name of prescription data
* `select`: the names of the columns to select
* `rename`: new column names; the default are the names required for the underlying functions, `processErx` and `processErxAddl`

In the above example, daily dose was calculated for the first 5 patients by multiplying strength\*dose\*freq.num, and a redundant daily dose was removed for the patient with ID2. In order to calculate a daily dose for the patient with ID3, the strength of 100 from the description was used because STRENGTH_AMOUNT was missing. For the patient with ID6, the dose amounts of 1.5, 1, and 1.5 are added together to get a dose of 4, and the daily dose is calculated as strength\*dose.


## Pro-Drug Level
Pro-Drug Level module processes drug concentration data that can be merged with medication dose data and other types of data. This module is semi-interactive; it generates several files while processing in order to check missing data and potential data errors, and get feedback from an investigator. If corrected information ('fix' files) are provided, the module should be re-run to incorporate the corrections. The major functions of this module are:

* Combine drug concentration data with sampling time: This step is necessary only if the drug concentration data file does not contain the sampling time (i.e., the time when blood samples were drawn for drug concentration measurements). When this is the case, the sampling time should be obtained from a separate data file.

* Check missing date/time for drug level measurements. If there are any missing dates, a file listing these records will be produced using the `failmiss_fn` stub appended with drugname. A 'fix' file can be added to correct these records. If no 'fix' file is provided, records with missing dates will be removed.
     + e.g., if `failmiss_fn` is 'MissingConcDate-', the file 'failMissingConcDate-dex.csv' will be created in the `check.path` directory. The file with corrected dates and times should replace 'fail' with 'fix' (e.g., 'fixMissingConcDate-dex.csv'), and be placed in the same `check.path` directory.


* Check for multiple sets of concentration data for the same subject, and keep the set of concentration data with the most drug concentration records above a lower limit of quantification (LLOQ). Data for subjects with multiple records, if any, is included in a file produced with the `multsets_fn` stub appended with the drugname and date.
     + e.g., if `multsets_fn` is 'multipleSetsConc-', the file 'multipleSetsConc-dex2020-07-15.csv' will be created in the `check.path` directory.


* Check duplicate concentration records on the same date/time. If there are any records with multiple concentrations on the same date/time, a file listing these records will be produced using the `faildup_fn` stub appended with drugname. A 'fix' file can be added to correct these records. If no 'fix' file is found, all duplicates are retained. 
     + e.g., if `faildup_fn` is 'DuplicateConc-', the file 'failDuplicateConc-dex.csv' will be created in the `check.path` directory.
The corrected file should replace 'fail' with 'fix', e.g. 'fixDuplicateConc-dex.csv', and include an additional variable called 'flag' with the value 'keep' for the records to be retained.

The drug concentration data can be processed by the `run_DrugLevel` function using:

```{r Pro-Drug-Level, eval=FALSE}
conc.out <- run_DrugLevel(conc.path=file.path(dataDir,"Fentanyl_conc_mod_id.rds"),
    conc.select=c('mod_id','mod_id_visit','samp','fentanyl_calc_conc'),
    conc.rename=c(fentanyl_calc_conc = 'conc.level', samp= 'event'),
    conc.mod.list=list(mod_id_event = expression(paste(mod_id_visit, event, sep = '_'))),
    samp.path=file.path(dataDir,"Fentanyl_samp_mod_id.rds"),
    samp.mod.list=list(mod_id_event = expression(paste(mod_id_visit, samp, sep = '_'))),
    check.path=checkDir,
    failmiss_fn = 'MissingConcDate-',
    multsets_fn = 'multipleSetsConc-',
    faildup_fn = 'DuplicateConc-',
    drugname=drugname,
    LLOQ=LLOQ,
    demo.list=demo.out)
head(conc.out)
```

```{r Pro-Drug-Level-fake, cache=TRUE, echo=FALSE}
co({
conc.out <- run_DrugLevel(conc.path=file.path(dataDir,"Fentanyl_conc_mod_id.rds"),
    conc.select=c('mod_id','mod_id_visit','samp','fentanyl_calc_conc'),
    conc.rename=c(fentanyl_calc_conc = 'conc.level', samp= 'event'),
    conc.mod.list=list(mod_id_event = expression(paste(mod_id_visit, event, sep = '_'))),
    samp.path=file.path(dataDir,"Fentanyl_samp_mod_id.rds"),
    samp.mod.list=list(mod_id_event = expression(paste(mod_id_visit, samp, sep = '_'))),
    check.path=checkDir,
    failmiss_fn = 'MissingConcDate-',
    multsets_fn = 'multipleSetsConc-',
    faildup_fn = 'DuplicateConc-',
    drugname=drugname,
    LLOQ=LLOQ,
    demo.list=demo.out)
})
```

```{r Pro-Drug-Level-out}
head(conc.out)
```

The output provides a message that 3 rows are missing concentration date. The file 'failMissingConcDate-fent.csv' contains the 3 records with missing values for the `date.time` variable.

```{r faildate,cache=TRUE}
( fail.miss.conc.date <- read.csv(file.path(checkDir,"failMissingConcDate-fent.csv")) )
```

We can correct the missing dates by providing an updated file called 'fixMissingConcDate-fent.csv' that contains the missing data.

```{r fixdate, cache=TRUE}
fail.miss.conc.date[,"date.time"] <- c("9/30/2016 09:32","10/1/2016 19:20","10/2/2016 02:04")
fail.miss.conc.date
 
write.csv(fail.miss.conc.date, file.path(checkDir,"fixMissingConcDate-fent.csv"))
```

After providing the updated file, the same `run_DrugLevel()` function should be re-run. The output now contains an additional message below the first message saying "fixMissingConcDate-fent.csv read with failures replaced". The conc.out data.frame also contains 3 additional rows with the corrected data.

```{r Pro-Drug-Level-rerun, eval=FALSE}
conc.out <- run_DrugLevel(conc.path=file.path(dataDir,"Fentanyl_conc_mod_id.rds"),
    conc.select=c('mod_id','mod_id_visit','samp','fentanyl_calc_conc'),
    conc.rename=c(fentanyl_calc_conc = 'conc.level', samp= 'event'),
    conc.mod.list=list(mod_id_event = expression(paste(mod_id_visit, event, sep = '_'))),
    samp.path=file.path(dataDir,"Fentanyl_samp_mod_id.rds"),
    samp.mod.list=list(mod_id_event = expression(paste(mod_id_visit, samp, sep = '_'))),
    check.path=checkDir,
    failmiss_fn = 'MissingConcDate-',
    multsets_fn = 'multipleSetsConc-',
    faildup_fn = 'DuplicateConc-', 
    drugname=drugname,
    LLOQ=LLOQ,
    demo.list=demo.out)
```

```{r Pro-Drug-Level-rerun-fake, cache=TRUE, echo=FALSE}
co({
conc.out <- run_DrugLevel(conc.path=file.path(dataDir,"Fentanyl_conc_mod_id.rds"),
    conc.select=c('mod_id','mod_id_visit','samp','fentanyl_calc_conc'),
    conc.rename=c(fentanyl_calc_conc = 'conc.level', samp= 'event'),
    conc.mod.list=list(mod_id_event = expression(paste(mod_id_visit, event, sep = '_'))),
    samp.path=file.path(dataDir,"Fentanyl_samp_mod_id.rds"),
    samp.mod.list=list(mod_id_event = expression(paste(mod_id_visit, samp, sep = '_'))),
    check.path=checkDir,
    failmiss_fn = 'MissingConcDate-',
    multsets_fn = 'multipleSetsConc-',
    faildup_fn = 'DuplicateConc-', 
    drugname=drugname,
    LLOQ=LLOQ,
    demo.list=demo.out)
})
```

```{r remove-fix, include=FALSE}
# remove fix file, so running vignette produces warning with first run of run_DrugLevel()
fx <- file.path(checkDir,"fixMissingConcDate-fent.csv")
if (file.exists(fx)) file.remove(fx)

# remove multiplesetsconc file
ms <- file.path(checkDir,paste0("multipleSetsConc-", drugname, Sys.Date(),".csv"))
if (file.exists(ms)) file.remove(ms)
```

* The `run_DrugLevel` function arguments are as follows:
    + `samp.path`: file path where the sampling time data exist
    + `samp.mod.list`: list containing modifications to variables in the sampling time data
    + `conc.path`: file path where the drug concentration data exist
    + `conc.select`: list of variables in the drug concentration data, which are used for processing
    + `conc.rename`: rename variables for the variables in conc.select
    + `conc.mod.list`: list containing modifications to variables in the drug concentration data
    + `check.path`: file path where the generated files for data checking are stored, and the corresponding data files with fixed data exist
    + `failmiss_fn`: filename stub for missing concentration date check file
    + `multsets_fn`: filename stub for multiple sets of concentration data check file
    + `faildup_fn`: filename stub for duplicate concentration check file
    + `drugname`: drug name of interest (e.g., dex, fent)
    + `LLOQ`: lower limit of quantification (LLOQ)
         - e.g., dexmedetomidine 0.005 ng/mL, fentanyl 0.05 ng/mL
    + `demo.list`: file name for processed demographic file that is used to exclude subjects based on exclusion criteria
* The output of the `run_DrugLevel` function is a dataset for processed drug concentration levels (conc.level) matched with date/time (date.time), as well as necessary identification numbers for further merging with the output from other modules.

## Pro-Laboratory

The Pro-Laboratory module processes laboratory data that can be merged with data from other modules. The laboratory data can be processed using:

```{r Pro-Laboratory, cache=TRUE, eval=TRUE}
creat.out <- run_Labs(lab.path=file.path(dataDir,"Fentanyl_creat_mod_id.rds"),
    lab.select = c('mod_id','date.time','creat'),
    lab.mod.list = list(date.time = expression(parse_dates(fixDates(paste(date, time))))))

alb.out <- run_Labs(lab.path=file.path(dataDir,"Fentanyl_alb_mod_id.rds"),
    lab.select = c('mod_id','date.time','alb'),
    lab.mod.list = list(date.time = expression(parse_dates(fixDates(paste(date, time))))))

lab.out <- list(creat.out, alb.out)

str(lab.out)
```

* The `run_Labs` function arguments are as follows:
    + `lab.path`: file path where the laboratory data exist
    + `lab.select`: list of variables in the laboratory data to be retained
    + `lab.mod.list`: list containing modifications to variables in the laboratory data

## Build-PK-IV

The Build-PK-IV module creates PK data for IV medications. Both dose data from the *Pro-Med-Str1* module and concentration data from the *Pro-DrugLevel* module are required. Demographic data from the *Pro-Demographic* module and laboratory data from the *Pro-Laboratory* module may optionally be included. The module is semi-interactive; it generates several files to check potential data errors, and get feedback from an investigator. If corrected information (‘fix’ files) are provided, the module should be re-run to incorporate the corrections. The major functions this module performs are:

* Determine whether each IV dose is valid by comparing to concentration data. Doses outside the time frame window defined by concentration data (by default seven days before first concentration through last concentration) are dropped. See `pkdata::trimDoses()` for more information.

* Resolve duplicate doses. A file with duplicate bolus dose records, if any, will be produced using the `faildupbol_fn` stub appended with drugname. A ‘fix’ file can be added to correct these records.
     + e.g., if `faildupbol_fn` is 'DuplicateBolus-', the file 'failDuplicateBolus-fent.csv' will be created in the `check.path` directory. The corrected file should replace 'fail' with 'fix' (e.g., 'fixDuplicateBolus-fent.csv'), and include an additional variable called 'flag' with the value 'keep' for the records to be retained.

* Add zero dose values after a gap.

* If demographic data is provided, update the 'maxint' value.

* Combine dose and concentration data into PK data format.

* If laboratory data is provided, merge onto PK data based on ID and time.

* If demographic data is provided, merge onto PK data. Demographic weight is used to impute records with missing dose weight. In addition, subjects who meet the exclusion criteria and those with no demographic data are removed. A file showing the missingness frequency and percent for each variable is produced using the `missdemo_fn` stub appended wih drugname.
     + e.g., if `missdemo_fn` is '-missing-demo', the file 'dex-missing-demo.csv' will be created in the `check.path` directory. 

* The final PK data includes ID and standard NONMEM formatted variables:
  + time - time of dosing or concentration event
  + conc - observed concentration (NA for dosing records)
  + amt - dose amount administered (NA for concentration records)
  + rate - rate of drug administration (e.g., rate=0 for bolus doses)
  + mdv - missing dependent variable (dv) indicator (e.g., 0 = not missing dv, 1 = missing dv)
  + evid - event ID (e.g., 0 = observation, 1 = dose event)
  
If demographic data is provided, the demographic variables named in `demo.vars` will also be included and renamed according to `demo.abbr`.

PK data with IV dosing can be built by the `run_Build_PK_IV` function using: 

```{r Build-PK-IV, eval=FALSE}
pk_dat <- run_Build_PK_IV(conc=conc.out,
    dose=ivdose.out,
    demo.list=demo.out,
    demo.vars=c('weight', 'weight_demo', 'height', 'gender',
                'ageatsurgery', 'stat_sts', 'cpb_sts',
                'length_of_icu_stay'),
    demo.abbr=c('wgt', 'wgt_demo', 'height', 'gender',
                'age', 'stat', 'cpb', 'loi'),
    lab.dat = lab.out,
    lab.vars = c('creat','alb'),
    pk.vars=c('mod_id_visit', 'time', 'conc', 'dose', 'rate', 'event',
              'other', 'multiple.record', 'date', 'mod_id'),
    drugname=drugname,
    check.path=checkDir,
    missdemo_fn='-missing-demo',
    faildupbol_fn='DuplicateBolus-',
    date.format="%m/%d/%y %H:%M:%S",
    date.tz="America/Chicago")
```

```{r Build-PK-IV-fake, cache=TRUE, echo=FALSE}
co({
pk_dat <- run_Build_PK_IV(conc=conc.out,
    dose=ivdose.out,
    demo.list=demo.out,
    demo.vars=c('weight', 'weight_demo', 'height', 'gender',
                'ageatsurgery', 'stat_sts', 'cpb_sts',
                'length_of_icu_stay'),
    demo.abbr=c('wgt', 'wgt_demo', 'height', 'gender',
                'age', 'stat', 'cpb', 'loi'),
    lab.dat = lab.out,
    lab.vars = c('creat','alb'),
    pk.vars=c('mod_id_visit', 'time', 'conc', 'dose', 'rate', 'event',
              'other', 'multiple.record', 'date', 'mod_id'),
    drugname=drugname,
    check.path=checkDir,
    missdemo_fn='-missing-demo',
    faildupbol_fn='DuplicateBolus-',
    date.format="%m/%d/%y %H:%M:%S",
    date.tz="America/Chicago")
})
```

The function `pullRealId()` appends the original IDs -- `subject_id` and `subject_uid` to the data. The parameter `remove.mod.id=TRUE` can be used to also remove any module IDs -- `mod_id`, `mod_visit`, and `mod_id_visit`. 

```{r Build-PK-IV-out}
# convert id back to original IDs
pk_dat <- pullRealId(pk_dat, remove.mod.id=TRUE)

head(pk_dat)
```

* The `run_Build_PK_IV` function arguments are as follows:
    + `conc`: concentration data output from run_DrugLevel (required)
    + `dose`: IV dose data output from run_MedStrI (required)
    + `demo.list`: (optional) file name for processed demographic file
    + `demo.vars`: (optional) vector of demographic variables to include in final output
    + `demo.abbr`: (optional) vector of abbreviations for demographic variables in `demo.vars`
    + `lab.dat`: (optional) list containing processed laboratory files
    + `lab.vars`: (optional) vector containing names for laboratory files
    + `pk.vars`: PK variables to include
    + `drugname`: drug name stub (e.g., fent)
    + `check.path`: file path where the generated files for data checking are stored, and the corresponding data files with fixed data exist
    + `missdemo_fn`: filename stub for report of missingness frequency and percent for variables
    + `faildupbol_fn`: filename stub for duplicate bolus dose records
    + `date.format`: date and time format  (e.g., "%m/%d/%y %H:%M:%S")
    + `date.tz`: date timezone (e.g., "America/Chicago")
* The output of the `run_MedStrI` function is a dataset with NONMEM formatted PK variables and (optionally) demographic and laboratory data.

## References  

1. Choi L, Beck C, McNeer E, Weeks HL, Williams ML, James NT, Niu X, Abou-Khalil BW, Birdwell KA, Roden DM, Stein CM. Development of a System for Post-marketing Population Pharmacokinetic and Pharmacodynamic Studies using Real-World Data from Electronic Health Records. Clinical Pharmacology & Therapeutics. 2020 Apr; 107(4): 934-943.  
